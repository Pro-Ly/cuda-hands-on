\section{并行直方图：原子操作和私有化简介}
到目前为止，我们提出的并行计算模式都允许将计算每个输出元素的任务专门分配给线程或由线程拥有。 因此，这些模式符合所有者计算规则，其中每个线程都可以写入其指定的输出元素，而无需担心其他线程的干扰。 本章介绍并行直方图计算模式，其中每个输出元素都可以由任何线程更新。 因此，在更新输出元素时必须注意线程之间的协调，并避免任何可能破坏最终结果的干扰。 在实践中，还有许多其他重要的并行计算模式，其中输出干扰无法轻易避免。 因此，并行直方图算法提供了这些模式中发生的输出干扰的示例。 我们将首先检查使用原子操作序列化每个元素的更新的基线方法。 这种基线方法简单但效率低下，常常导致执行速度令人失望。 然后，我们将介绍一些广泛使用的优化技术，尤其是私有化，以显着提高执行速度，同时保持正确性。 这些技术的成本和收益取决于底层硬件以及输入数据的特征。 因此，对于开发人员来说，理解这些技术的关键思想并能够推理它们在不同情况下的适用性非常重要。

\subsection{背景}
直方图是数据集中数据值的计数或出现百分比的显示。 在最常见的直方图形式中，值区间沿水平轴绘制，每个区间中的数据值计数表示为从水平轴上升的矩形或条形的高度。 例如，直方图可用于显示短语“对大规模并行处理器进行编程”中字母表中字母的频率。 为简单起见，我们假设输入短语全部为小写。 通过检查，我们看到字母“a”有四个实例，字母“b”有零个实例，字母“c”有一个实例，依此类推。 我们将每个值区间定义为四个字母的连续范围。 因此，第一个值区间是“a”到“d”，第二个值区间是“e”到“h”，依此类推。 图 9.1 显示了一个直方图，根据我们对值区间的定义，显示了短语“编程大规模并行处理器”中字母的频率。

直方图提供了有用的数据集摘要。 在我们的示例中，我们可以看到所表示的短语由大量集中在字母表中间间隔的字母组成，而在后面的间隔中明显稀疏。 直方图的形状有时被称为数据集的特征，并提供一种快速方法来确定数据集中是否存在显着现象。 例如，购买类别的直方图形状和信用卡帐户的位置可用于检测欺诈使用。 当直方图的形状明显偏离标准时，系统会提出潜在问题的标志。

许多应用领域依赖直方图来汇总数据集以进行数据分析。 计算机视觉就是这样的领域之一。 不同类型的物体图像（例如人脸与汽车）的直方图往往呈现不同的形状。 例如，可以绘制图像或图像区域中像素发光值的直方图。 这样的晴天天空直方图在发光光谱的高值区间中可能只有少量非常高的条形。 通过将图像划分为子区域并分析这些子区域的直方图，人们可以快速识别图像中可能包含感兴趣对象的感兴趣的子区域。 计算图像子区域直方图的过程是计算机视觉中特征提取的重要方法，其中特征指的是图像中感兴趣的模式。 在实践中，每当需要分析大量数据以提取有趣的事件时，直方图就可能被用作基础计算。 信用卡欺诈检测和计算机视觉显然符合这一描述。 具有此类需求的其他应用领域包括语音识别、网站购买推荐和科学数据分析，例如关联天体物理学中的天体运动。

可以轻松地以顺序方式计算直方图。 图 9.2 显示了计算图 9.1 中定义的直方图的顺序函数。 为简单起见，直方图函数需要仅识别小写字母。 C 代码假设输入数据集采用 char 数组数据，并且直方图将生成到 int 数组 histo 中（第 01 行）。 输入数据项的数量在函数参数长度中指定。 for 循环（第 02-07 行）顺序遍历数组，识别已访问位置 data[i] 中字符的字母表索引，将字母表索引保存到 Alphabet\_position 变量中，并递增与关联的 histo[alphabet\_position/4] 元素 那个间隔。 字母索引的计算依赖于这样一个事实：输入字符串基于标准 ASCII 代码表示，其中字母“a”到“z”根据其在字母表中的顺序被编码为连续值。

尽管人们可能不知道每个字母的确切编码值，但可以假设字母的编码值是“a”的编码值加上该字母与“a”之间的字母位置差。 在输入中，每个字符都存储在其编码值中。 因此，表达式 data[i] À “a”（第 03 行）导出字母的字母表位置，其中“a”的字母表位置为 0。如果位置值大于或等于 0 且小于 26，则 数据字符确实是一个小写字母（第 04 行）。 请记住，我们定义了间隔，以便每个间隔包含四个字母。 因此，字母的间隔索引是其字母表位置值除以 4。我们使用间隔索引来增加适当的 histo 数组元素（第 05 行）。

图 9.2 中的 C 代码非常简单且高效。 该算法的计算复杂度为 O(N)，其中 N 是输入数据元素的数量。 数据数组元素在 for 循环中按顺序访问，因此每当从系统 DRAM 获取数据时，CPU 缓存线都会得到很好的利用。 histo 数组非常小，非常适合 CPU 的一级 (L1) 数据缓存，从而确保 histo 元素的快速更新。 对于大多数现代 CPU，人们可以预期该代码的执行速度受到内存限制，即受到数据元素从 DRAM 进入 CPU 缓存的速率的限制。

\subsection{原子操作和基础直方图核函数}
并行化直方图计算的最直接方法是启动与数据元素一样多的线程，并让每个线程处理一个输入元素。 每个线程读取其分配的输入元素并递增该字符的适当间隔计数器。 图 9.3 展示了这种并行化策略的一个例子。 请注意，多个线程需要更新同一个计数器（m-p），这是一种冲突，称为输出干扰。 程序员必须了解竞争条件和原子操作的概念，以便自信地处理并行代码中的此类输出干扰。

histo 数组中间隔计数器的增量是对内存位置的更新或读修改写操作。 该操作包括读取内存位置（读）、将原始值加一（修改）以及将新值写回内存位置（写）。 读-修改-写是协调协作活动的常用操作。

例如，当我们向航空公司预订航班时，我们会调出座位图并查找可用座位（读取），选择要预订的座位（修改），然后将座位图中的座位状态更改为不可用 （写）。 可能发生的不良情况如下：

• 两名乘客同时调出同一航班的座位图。

• 两位顾客选择相同的座位，例如9C。

• 两位客户均将座位9C 的状态更改为不可用。

排序结束后，两位顾客都认为自己的座位是 9C。 可以想象，当他们登机时，发现其中一人不能坐在预定的座位上，会有一种不愉快的情况！ 不管你信不信，这种不愉快的情况在现实生活中时有发生，都是因为机票预订软件的缺陷造成的。

再比如，有些商店允许顾客无需排队等待服务。 他们要求每位顾客从其中一个售货亭获取一个号码。 有一个显示屏显示接下来要服务的号码。 当服务代理有空时，代理会要求客户出示与号码匹配的票据，验证票据，并将显示号码更新为下一个更大的号码。 理想情况下，所有顾客都将按照他们进入商店的顺序获得服务。 不希望出现的结果是两个客户同时在两个自助服务终端登录，并且都收到具有相同号码的门票。 当服务代理拨打该号码时，两位客户都希望自己是应该获得服务的人。

在这两个示例中，不良结果都是由称为“读取-修改-写入竞争条件”的现象引起的，其中两个或多个同时更新操作的结果根据所涉及操作的相对时序而变化。 1 有些结果是正确的，有些结果是错误的。 图 9.4 说明了当两个线程尝试更新我们的文本直方图示例中的相同 histo 元素时的竞争条件。 图 9.4 中的每一行显示了一段时间内的活动，时间从上到下进行。

图 9.4A 描述了这样一种场景，其中线程 1 在时间段 1 到 3 期间完成其读取-修改-写入序列的所有三个部分，然后线程 2 在时间段 4 开始其序列。每个操作前面括号中的值 显示写入目标的值，假设 histo[x] 的值最初为 0。在这种情况下，histo[x] 的值随后为 2，这正是人们所期望的。 也就是说，两个线程都成功增加了 histo[x]。 元素值从0开始，运算完成后变为2。

在图 9.4B 中，两个线程的读取-修改-写入序列重叠。 请注意，线程 1 在时间段 4 将新值写入 histo[x]。当线程 2 在时间段 3 读取 histo[x] 时，它的值仍然为 0。因此，它计算出的新值最终 写入 histo[x] 的是 1 而不是 2。问题是线程 2 在线程 1 完成其更新之前过早读取 histo[x]。 最终结果是 histo[x] 之后的值为 1，这是不正确的。 线程 1 的更新丢失。

在并行执行期间，线程可以按相对于彼此的任何顺序运行。 在我们的示例中，线程 2 可以轻松地在线程 1 之前启动其更新序列。图 9.5 显示了两个这样的场景。 在图 9.5A 中，线程 2 在线程 1 开始更新之前完成更新。 在图 9.5B 中，线程 1 在线程 2 完成之前开始更新。 显然，图 9.5A 中的序列会产生 histo[x] 的正确结果，但图 9.5B 中的序列会产生错误的结果。

histo[x] 的最终值根据所涉及操作的相对时间而变化，这一事实表明存在竞争条件。 我们可以通过消除线程1和线程2的操作序列可能的交错来消除这种变化。也就是说，我们希望允许图1和图2所示的时序。 9.4A和9.5A，同时消除图9.4A和9.5A中所示的可能性。 9.4B和9.5B。 这可以通过使用原子操作来完成。

对内存位置的原子操作是在内存位置上执行读取-修改-写入序列的操作，其方式是对该位置的其他读取-修改-写入序列不能与其重叠。 也就是说，操作的读取、修改和写入部分形成一个不可分割的单元，因此称为原子操作。 实际上，原子操作是通过硬件支持来实现的，以将其他操作锁定到同一位置，直到当前操作完成。 在我们的示例中，这种支持消除了图 1 和 2 中描述的可能性。 9.4B 和 9.5B，因为在引导线程完成其更新序列之前，尾随线程无法开始其更新序列。

重要的是要记住，原子操作不会在线程之间强制执行任何特定的执行顺序。 在我们的示例中，图 1 和 2 中显示的两个顺序。 9.4A 和 9.5A 是原子操作所允许的。 线程 1 可以在线程 2 之前或之后运行。正在强制执行的规则是，如果两个线程在同一内存位置上执行原子操作，则尾随线程执行的原子操作无法启动，直到线程 2 的原子操作完成为止。 主导线程完成。 这有效地序列化了在内存位置上执行的原子操作。

原子操作通常根据对内存位置执行的修改来命名。 在我们的文本直方图示例中，我们向内存位置添加一个值，因此原子操作称为原子添加。 其他类型的原子操作包括减法、递增、递减、最小值、最大值、逻辑与和逻辑或。 CUDA 内核可以通过函数调用对内存位置执行原子添加操作：

atomicAdd函数是一个内部函数（参见侧边栏“内部函数”），被编译成硬件原子操作指令。 该指令读取全局或共享内存中地址参数指向的 32 位字，将 val 添加到旧内容，并将结果存储回内存中的同一地址。 该函数返回该地址处的旧值。

图 9.6 显示了执行并行直方图计算的 CUDA 内核。 该代码与图 9.2 中的顺序代码类似，但有两个关键区别。 第一个区别是输入元素上的循环被线程索引计算（第 02 行）和边界检查（第 03 行）替换，以将线程分配给每个输入元素。 第二个区别是图9.2中的增量表达式：

变成图 9.6 中的atomicAdd()函数调用（第06行）。 要更新的位置的地址 \&(histo[alphabet\_position/4]) 是第一个参数。要添加到位置的值 1 是第二个参数。 这确保了不同线程对任何 histo 数组元素的任何同时更新都被正确序列化。

\subsection{原子操作的延迟和吞吐量}
图 9.6 的内核中使用的原子操作通过将任何同时更新序列化到某个位置来确保更新的正确性。 众所周知，串行化大规模并行程序的任何部分都会大大增加执行时间并降低程序的执行速度。 因此，此类串行操作占用尽可能少的执行时间非常重要。

正如我们在第 5 章“内存架构和数据局部性”中了解到的，DRAM 中数据的访问延迟可能需要数百个时钟周期。 在第 4 章“计算架构和调度”中，我们了解到 GPU 使用零周期上下文切换来容忍此类延迟。 在第 6 章“性能注意事项”中，我们了解到，只要有许多内存访问延迟可以相互重叠的线程，执行速度就会受到内存系统吞吐量的限制。 因此，GPU 充分利用 DRAM 突发、存储体和通道来实现高内存访问吞吐量非常重要。

此时读者应该清楚，高内存访问吞吐量的关键是同时进行许多 DRAM 访问。 不幸的是，当许多原子操作更新同一内存位置时，这种策略就会失效。 在这种情况下，在引导线程的读取-修改-写入序列完成之前，尾随线程的读取-修改-写入序列无法启动。 如图 9.7 所示，在同一内存位置执行原子操作时只能有一个正在进行。 每个原子操作的持续时间大约是内存加载的延迟（原子操作时间的左侧部分）加上内存存储的延迟（原子操作时间的右侧部分）。 每个读取-修改写入操作的这些时间部分的长度（通常为数百个时钟周期）定义了必须专用于服务每个原子操作的最小时间量，并限制了吞吐量或可以执行原子操作的速率。

例如，假设存储器系统每通道具有 64 位（8 字节）双倍数据速率 DRAM 接口、八个通道、1 GHz 时钟频率以及 200 个周期的典型访问延迟。 内存系统的峰值访问吞吐量为 8（字节/传输）× 2（每个通道每个时钟的传输数）× 1 G（每秒时钟数）× 8（通道）= 128 GB/s。 假设每个访问的数据是4字节，系统

tem 的峰值访问吞吐量为每秒 32 G 数据元素。

然而，在特定内存位置执行原子操作时，可以实现的最高吞吐量是每 400 个周期执行一次原子操作（200 个周期用于读取，200 个周期用于写入）。 这意味着基于时间的吞吐量为 1/400 原子/时钟 × 1 G（时钟/秒）= 2.5 M 原子/秒。 这大大低于大多数用户对 GPU 内存系统的期望。 此外，原子操作序列的长延迟可能会主导内核执行时间，并可能显着降低内核的执行速度。

实际上，并非所有原子操作都将在单个内存位置上执行。 在我们的文本直方图示例中，直方图有七个间隔。 如果输入字符均匀分布在字母表中，则原子操作将均匀分布在 histo 元素中。 这会将吞吐量提高到每秒 7 × 2.5 M = 17.5 M 原子操作。 实际上，提升因子往往比直方图中的间隔数低得多，因为字符在字母表中往往有偏差分布。 例如，在图 9.1 中，我们看到示例短语中的字符严重偏向 m-p 和 q-t 间隔。 更新这些间隔的大量争用流量可能会将可实现的吞吐量降低到仅 (28/10) × 2.5 M = 7 M 左右。

提高原子操作吞吐量的一种方法是减少对竞争激烈的位置的访问延迟。 高速缓冲存储器是减少存储器访问延迟的主要工具。 因此，现代 GPU 允许在最后一级缓存中执行原子操作，该缓存在所有流式多处理器 (SM) 之间共享。 在原子操作过程中，如果在末级缓存中找到更新的变量，则在缓存中更新该变量。 如果在最后一级缓存中找不到，则触发缓存未命中，并被带入缓存，并在那里进行更新。 由于通过原子操作更新的变量往往会被许多线程大量访问，因此这些变量一旦从 DRAM 引入，往往会保留在缓存中。 由于末级缓存的访问时间在数十个周期而不是数百个周期，因此原子操作的吞吐量比前几代GPU至少提高了一个数量级。 这是大多数现代 GPU 支持末级缓存中原子操作的重要原因。

