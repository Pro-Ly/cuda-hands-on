\section{异构计算集群编程：CUDA stream 简介}
到目前为止，我们专注于对一台主机和一台设备的异构计算系统进行编程。 在高性能计算（HPC）中，应用程序需要计算节点集群的聚合计算能力。 如今，许多 HPC 集群的每个节点都有一台或多台主机和一台或多台设备。 从历史上看，这些集群主要使用消息传递接口 (MPI) 进行编程。 在本章中，我们将介绍 MPI/CUDA 联合编程。 我们将仅介绍程序员需要了解的 MPI 概念，以将其异构应用程序扩展到集群环境中的多个节点。 特别是，我们将重点关注将 CUDA 内核扩展到多个节点的背景下的域分区、点对点通信和集体通信。

\subsection{背景}
尽管在 2009 年之前几乎没有顶级超级计算机使用 GPU，但对更高能源效率的需求导致近年来 GPU 的快速采用。 当今世界上许多顶级超级计算机在每个节点中都使用 CPU 和 GPU。 他们在 Green500 名单中的高排名验证了这种方法的有效性，这反映了他们的高能源效率。

当今计算集群的主导编程接口是 MPI（Gropp 等人，1999），它是一组 API 函数，用于计算集群中运行的进程之间的通信。 MPI 采用分布式内存模型，其中进程通过相互发送消息来交换信息。 当应用程序使用API通信功能时，不需要处理互连网络的细节。 MPI 实现允许进程使用逻辑号码相互寻址，这与在电话系统中使用电话号码的方式非常相似：电话用户可以使用电话号码互相拨号，而无需确切知道被叫方在哪里以及呼叫的情况。 路由。

在典型的 MPI 应用程序中，数据和工作在进程之间划分。 如图 20.1 所示，每个节点可以包含一个或多个进程，如图 20.1 所示，节点内显示为云。 随着这些过程的进展，它们可能需要彼此的数据。 这种需求可以通过发送和接收消息来满足。 在某些情况下，这些流程还需要在协作执行大型任务时相互同步并生成集体结果。 这是通过集体通信 API 函数完成的。

\subsection{一个运行的例子}

\subsection{总结}
在本章中，我们介绍了具有异构计算节点的 HPC 集群的 CUDA/MPI 联合编程的基本模式。 MPI 应用程序中的所有进程都运行相同的程序。 但是，每个进程可以遵循不同的控制流和函数调用路径来专门化其角色，如我们示例中的数据服务器和计算进程所示。 我们还使用模板模式来展示计算过程如何交换数据。 我们提出了使用 CUDA 流和异步数据传输来实现计算和通信的重叠。 我们还展示了如何使用 MPI 屏障来确保所有进程都准备好相互交换数据。 最后，我们简要概述了使用 CUDA 感知 MPI 来简化设备内存中的数据交换。 我们想指出，虽然 MPI 是一个非常不同的编程系统，但我们在本章中介绍的所有主要 MPI 概念，即 SPMD、MPI 等级和障碍，在 CUDA 编程模型中都有对应的概念。 这证实了我们的信念：通过使用一种模型很好地教授并行编程，我们的学生可以快速轻松地掌握其他编程模型。 我们希望鼓励读者在本章提供的基础上学习更高级的 MPI 功能和其他重要模式。