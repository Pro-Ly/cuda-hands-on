\section{深度学习}
本章介绍了深度学习的应用案例研究，深度学习是使用人工神经网络的机器学习的最新分支。 机器学习已被用于许多应用领域，根据从数据集中收集的经验来训练或调整应用程序逻辑。 为了有效，人们通常需要使用大量数据进行此类训练。 虽然机器学习作为计算机科学的一个学科已经存在很长时间了，但由于两个原因，它最近在实际行业中获得了广泛的认可。 第一个原因是互联网的普遍使用提供了大量数据。 第二个原因是廉价的大规模并行 GPU 计算系统，可以利用这些海量数据集有效地训练应用程序逻辑。 我们将从机器学习和深度学习的简要介绍开始，然后更详细地考虑最流行的深度学习算法之一：卷积神经网络（CNN）。 CNN 具有较高的计算与内存访问比率和高水平的并行性，这使它们成为 GPU 加速的完美候选者。 我们将首先介绍卷积神经网络的基本实现。 接下来，我们将展示如何使用共享内存改进这个基本实现。 然后，我们将展示如何将卷积层表示为矩阵乘法，这可以通过在现代 GPU 中使用高度优化的硬件和软件来加速。

\subsection{背景}

\subsection{总结}
本章首先简要介绍了机器学习。 然后，它更深入地研究分类任务并引入感知器，这是一种线性分类器，是理解现代 CNN 的基础。 我们讨论了如何为单层和 MLP 实现前向推理和后向传播训练过程。 特别是，我们讨论了对可微激活函数的需求，以及在训练过程中如何通过多层感知器网络中的链式规则更新模型参数。

基于对感知器的概念和数学理解，我们提出了一个基本的卷积神经网络及其主要类型层的实现。 这些层可以被视为感知器的特殊情况和/或简单改编。 然后，我们在第 7 章“卷积”中的卷积模式的基础上，提出了卷积层（CNN 中计算最密集的层）的 CUDA 内核实现。

然后，我们提出了通过将输入特征映射展开为矩阵来将卷积层公式化为矩阵乘法的技术。 该转换使卷积层能够从针对 GPU 的高度优化的 GEMM 库中受益。 我们还介绍了输入矩阵展开过程的 C 和 CUDA 实现，并讨论了展开方法的优缺点。

我们在本章结束时概述了大多数深度学习框架都使用的 CUDNN 库。 这些框架的用户可以从高度优化的层实现中受益，而无需自己编写 CUDA 内核。