\section{稀疏矩阵计算}
我们的下一个并行模式是稀疏矩阵计算。 在稀疏矩阵中，大多数元素为零。 存储和处理这些零元素在内存容量、内存带宽、时间和能量方面都是浪费的。 许多重要的现实问题都涉及稀疏矩阵计算。 由于这些问题的重要性，几种稀疏矩阵存储格式及其相应的处理方法被提出并在该领域得到广泛应用。 所有这些方法都采用某种类型的压缩技术来避免存储或处理零元素，但代价是在数据表示中引入某种程度的不规则性。 不幸的是，这种不规则性可能导致并行计算中内存带宽的利用不足、控制流发散和负载不平衡。 因此，在压缩和正则化之间取得良好的平衡非常重要。 一些存储格式在高度不规则性的情况下实现了更高水平的压缩。 其他人实现了更适度的压缩水平，同时保持表示更规则。 众所周知，使用每种存储格式的并行计算的相对性能在很大程度上取决于稀疏矩阵中非零元素的分布。 了解稀疏矩阵存储格式及其相应并行算法的大量工作，为并行程序员在解决相关问题时应对压缩和正则化挑战提供了重要的背景。

\subsection{背景}

\subsection{总结}
在本章中，我们将稀疏矩阵计算作为一种重要的并行模式提出。 稀疏矩阵在许多涉及复杂现象建模的现实应用中非常重要。 此外，稀疏矩阵计算是许多大型实际应用程序依赖于数据的性能行为的一个简单示例。 由于零元素数量较多，因此使用压缩技术来减少对这些零元素执行的存储量、内存访问量和计算量。 使用这种模式，我们引入了使用混合方法和排序/分区的正则化概念。 这些正则化方法用于许多实际应用中。 有趣的是，一些正则化技术将零元素重新引入到压缩表示中。 我们使用混合方法来减轻可能引入过多零元素的病态情况。 读者可以参考 Bell 和 Garland (2009)，并鼓励读者尝试不同的稀疏数据集，以更深入地了解本章介绍的各种 SpMV 内核的数据依赖性能行为。

应该清楚的是，并行 SpMV 内核的执行效率和内存带宽效率都取决于输入数据矩阵的分布。 这与我们迄今为止研究的大多数内核有很大不同。 然而，这种依赖于数据的性能行为在实际应用中非常常见。 这就是并行 SpMV 如此重要的并行模式的原因之一。 它很简单，但它说明了许多复杂并行应用程序中的重要行为。

与稠密矩阵计算相比，我们想对稀疏矩阵计算的性能进行补充说明。 一般来说，稀疏矩阵计算通过 CPU 或 GPU 实现的 FLOPS 等级比密集矩阵计算低得多。 对于 SpMV 来说尤其如此，其中稀疏矩阵中没有数据重用。 OP/B 基本上为 0.25，将可实现的 FLOPS 率限制为峰值性能的一小部分。 各种格式对于 CPU 和 GPU 都很重要，因为两者在执行 SpMV 时都受到内存带宽的限制。 过去，人们常常对 CPU 和 GPU 上此类计算的低 FLOPS 评级感到惊讶。 读完这一章，你应该不再感到惊讶了。